{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project Ambition",
      "provenance": [],
      "collapsed_sections": [
        "FPXQ-z-SmXdE"
      ],
      "machine_shape": "hm",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/serhatataman/ProjectAmbitionColab/blob/main/Project_Ambition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Muy2Y5jKDipP"
      },
      "source": [
        "# Project Ambition\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJBs_flRovLc"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The type of GPU assigned to you by Colab will greatly affect your training time. Some sample times that I achieved with Colab are given here. I've found that Colab Pro generally starts you with a V100, however, if you run scripts non-stop for 24hrs straight for a few days in a row, you will generally be throttled back to a P100.\n",
        "\n",
        "*   1024x1024 - V100 - 566 sec/tick (CoLab Pro)\n",
        "*   1024x1024 - P100 - 1819 sec/tick (CoLab Pro)\n",
        "*   1024x1024 - T4 - 2188 sec/tick (CoLab Free)\n",
        "\n",
        "\n",
        "If you use Google CoLab Pro, generally, it will not disconnect before 24 hours, even if you (but not your script) are inactive. Free CoLab WILL disconnect a perfectly good running script if you do not interact for a few hours. The following describes how to circumvent this issue.\n",
        "\n",
        "\n",
        "Note: if this step gives `NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running` error, select GPU as hardware accelerator in `Edit > Notebook settings`."
      ],
      "metadata": {
        "id": "lPBybcggXMFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGZ8hikAV80H",
        "outputId": "e9d970df-2acb-4cf0-e5fd-4e1650afcb0c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Jan  8 01:44:23 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "V1G82GuO-tez",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bca9464-4c36-4359-d916-50f48bd147f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mount to the Google Drive\n",
        "```\n",
        "/content/drive/MyDrive/data\n",
        "```\n",
        "\n",
        "Use ```ls``` command to establish the exact path for your images."
      ],
      "metadata": {
        "id": "dx8alQk_XBWE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    COLAB = True\n",
        "    print(\"Using Google CoLab\")\n",
        "except:\n",
        "    print(\"Not using Google CoLab\")\n",
        "    COLAB = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeMVNvOyXFQW",
        "outputId": "d6c46f2e-d9c8-412e-c971-f6ddcbea0097"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Using Google CoLab\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xe_S5N3xZd6B",
        "outputId": "83fd8b51-9c28-4a9f-a02c-69e035d56f52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Colab Notebooks'\t\t\t  'Master Application Docs.'\n",
            "'Copy of [#1 Sharing Session] TSO.docx'    Personal\n",
            "'Copy of [#4 Re-Entry Session] TSO.docx'   ProjectAmbition\n",
            "'CS:GO config'\t\t\t\t  'Reflection groups guideline.gdoc'\n",
            " CV\t\t\t\t\t   tests\n",
            "'Graduation Thesis'\t\t\t   UDI\n",
            " ielts\t\t\t\t\t   Wallpapers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGKA5Q7uEbR2"
      },
      "source": [
        "\n",
        "### Import NVIDIA stylegan3\n",
        "\n",
        "If the repo is already installed, it will skip the installation process and change into the repoâ€™s directory. If not, it will install all the files necessary.\n",
        "Also, create `downloads` and `datasets` folders."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HX77jscX2zV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b54cee4-7c17-496e-dbca-c5a5954281d0"
      },
      "source": [
        "import os\n",
        "if os.path.isdir(\"/content/drive/MyDrive/ProjectAmbition\"):\n",
        "    %cd \"/content/drive/MyDrive/ProjectAmbition\"\n",
        "else:\n",
        "    #install script\n",
        "    %cd \"/content/drive/MyDrive/\"\n",
        "    !mkdir ProjectAmbition\n",
        "    %cd ProjectAmbition\n",
        "    !git clone https://github.com/NVlabs/stylegan3\n",
        "    !mkdir downloads\n",
        "    !mkdir datasets\n",
        "    !mkdir raw_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ProjectAmbition\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"/content/drive/MyDrive/ProjectAmbition/stylegan3\"\n",
        "!git config --global user.name \"serhatataman\"\n",
        "!git config --global user.email \"serhatataman13@hotmail.com\"\n",
        "!git fetch origin\n",
        "!git checkout origin/main -- train.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRItln1dee3m",
        "outputId": "97b5990d-ecf4-4270-c935-3d779e4628e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ProjectAmbition/stylegan3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Checking if directories/files exist"
      ],
      "metadata": {
        "id": "KLRvzXGrfF2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "if not os.path.exists(\"/content/drive/MyDrive/ProjectAmbition/stylegan3/dataset_tool.py\"):\n",
        "  raise FileNotFoundError(\"dataset_tool.py file does not exist!\")\n",
        "\n",
        "if not os.path.exists(\"/content/drive/MyDrive/ProjectAmbition/raw_data\"):\n",
        "  raise FileNotFoundError(\"raw_data folder does not exist!\")\n",
        "\n",
        "if not os.path.exists(\"/content/drive/MyDrive/ProjectAmbition/dataset\"):\n",
        "  print(\"dataset folder does not exist! creating the folder...\")\n",
        "  os.mkdir(\"/content/drive/MyDrive/ProjectAmbition/dataset\")\n",
        "\n",
        "if not os.path.exists(\"/content/drive/MyDrive/ProjectAmbition/output\"):\n",
        "  print(\"output folder does not exist! creating the folder...\")\n",
        "  os.mkdir(\"/content/drive/MyDrive/ProjectAmbition/output\")"
      ],
      "metadata": {
        "id": "5GK0kPaGflpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fetch images"
      ],
      "metadata": {
        "id": "ZW8BykZWxeCj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download from webscrap of WikiArt"
      ],
      "metadata": {
        "id": "R0zxbFebhphg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/ProjectAmbition/raw_data\"\n",
        "base_url = \"https://www.wikiart.org\"\n",
        "\n",
        "# iterate through all artists by last name alphabetically\n",
        "for c in range(ord('a'), ord('z') + 1):\n",
        "    char = chr(c)\n",
        "    artist_list_url = base_url + '/en/Alphabet/' + char + '/text-list'\n",
        "\n",
        "    genre_soup = BeautifulSoup(urllib.request.urlopen(artist_list_url), \"lxml\")\n",
        "    artist_list_main = genre_soup.find(\"main\")\n",
        "    lis = artist_list_main.find_all(\"li\")\n",
        "\n",
        "    # for each list element\n",
        "    for li in lis:\n",
        "\n",
        "        # get the date range\n",
        "        for line in li.text.splitlines():\n",
        "            # if line.startswith(\",\") and \"-\" in line:\n",
        "            #     parts = line.split('-')\n",
        "            #     if len(parts) == 2:\n",
        "            #         born = int(re.sub(\"[^0-9]\", \"\", parts[0]))\n",
        "            #         died = int(re.sub(\"[^0-9]\", \"\", parts[1]))\n",
        "\n",
        "            # look for artists who may have created work that could in public domain\n",
        "            # if born > 1850 and died > 0 and (born < 1900 or died < 1950):\n",
        "\n",
        "            link = li.find(\"a\")\n",
        "            if link is None:\n",
        "                continue\n",
        "\n",
        "            artist = link.attrs[\"href\"]\n",
        "\n",
        "            # get the artist's main page\n",
        "            artist_url = base_url + artist\n",
        "            artist_soup = BeautifulSoup(urllib.request.urlopen(artist_url), \"lxml\")\n",
        "\n",
        "            # only look for artists with the word abstract on their main page\n",
        "            if \"Abstract\" in artist_soup.text or \"abstract\" in artist_soup.text or \"Avant-garde\" \\\n",
        "                    in artist_soup.text or \"avant-garde\" in artist_soup.text:\n",
        "                print('Artist: ' + artist)\n",
        "\n",
        "                # get the artist's web page for the artwork\n",
        "                url = base_url + artist + '/all-works/text-list'\n",
        "\n",
        "                try:\n",
        "                    artist_work_soup = BeautifulSoup(urllib.request.urlopen(url), \"lxml\")\n",
        "                except:\n",
        "                    print(\"Error retrieving artist's work list. Url was: \" + url)\n",
        "                    continue\n",
        "\n",
        "                # get the main section\n",
        "                artist_main = artist_work_soup.find(\"main\")\n",
        "                image_count = 0\n",
        "                artist_name = artist.split(\"/\")[2]\n",
        "\n",
        "                # get the list of artwork\n",
        "                lis = artist_main.find_all(\"li\")\n",
        "\n",
        "                # for each list element\n",
        "                for li in lis:\n",
        "                    link = li.find(\"a\")\n",
        "\n",
        "                    if link != None:\n",
        "                        painting = link.attrs[\"href\"]\n",
        "\n",
        "                        # get the painting\n",
        "                        url = base_url + painting\n",
        "                        # print('Painting base url: ' + url)\n",
        "\n",
        "                        try:\n",
        "                            painting_soup = BeautifulSoup(urllib.request.urlopen(url), \"lxml\")\n",
        "\n",
        "                        except:\n",
        "                            print(\"error retrieving page\")\n",
        "                            continue\n",
        "\n",
        "                        # check the copyright\n",
        "                        if \"Public domain\" in painting_soup.text:\n",
        "\n",
        "                            # check the genre\n",
        "                            genre = painting_soup.find(\"span\", {\"itemprop\": \"genre\"})\n",
        "                            if genre != None and genre.text == \"abstract\":\n",
        "\n",
        "                                # get the url\n",
        "                                og_image = painting_soup.find(\"meta\", {\"property\": \"og:image\"})\n",
        "                                image_url = og_image[\"content\"].split(\"!\")[0]  # ignore the !Large.jpg at the end\n",
        "\n",
        "                                save_path = file_path + \"/\" + artist_name + \"_\" + str(image_count) + \".jpg\"\n",
        "\n",
        "                                # download the file\n",
        "                                try:\n",
        "                                    print(f\"Downloading {image_url} to {save_path}\")\n",
        "                                    time.sleep(0.2)  # try not to get a 403\n",
        "                                    urllib.request.urlretrieve(image_url, save_path)\n",
        "                                    image_count = image_count + 1\n",
        "                                except Exception as e:\n",
        "                                    print(\"Failed downloading \" + image_url, e)\n"
      ],
      "metadata": {
        "id": "9h9-uiDZxghm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download zip of art pieces"
      ],
      "metadata": {
        "id": "08p5Qj94vyad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "zip_file_url = 'http://web.fsktm.um.edu.my/~cschan/source/ICIP2017/wikiart.zip'\n",
        "\n",
        "# NOTE the stream=True parameter below\n",
        "with requests.get(zip_file_url, stream=True) as response:\n",
        "  response.raise_for_status()\n",
        "  handle = open('/content/drive/MyDrive/ProjectAmbition/downloads/data.zip', \"wb\")\n",
        "  for chunk in response.iter_content(chunk_size=8192): \n",
        "      # If you have chunk encoded response uncomment if\n",
        "      # and set chunk_size parameter to None.\n",
        "      #if chunk: \n",
        "      handle.write(chunk)\n",
        "\n",
        "  handle.close()\n",
        "\n",
        "# Note that the number of bytes returned using iter_content is not exactly the chunk_size;\n",
        "# it's expected to be a random number that is often far bigger, and is expected to be different in every iteration.\n",
        "# See body-content-workflow and Response.iter_content for further reference.\n",
        "\n",
        "print('Download completed...')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bM4CRE7Lhzc8",
        "outputId": "ff2c3d1d-a667-4f3b-ebdc-665bbfd65332"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download completed...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unzip downloaded files"
      ],
      "metadata": {
        "id": "LSrDe3eOyIuV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "zip_filename = \"/content/drive/MyDrive/ProjectAmbition/downloads/data.zip\"\n",
        "directory_to_extract_to = \"/content/drive/MyDrive/ProjectAmbition/downloads/\"\n",
        "\n",
        "with zipfile.ZipFile(zip_filename, \"r\") as zip_ref:\n",
        "    for name in zip_ref.namelist():\n",
        "        try:\n",
        "            zip_ref.extract(name, directory_to_extract_to)\n",
        "        except (Exception, zipfile.BadZipFile) as e:\n",
        "            print(\"A file is corrupted. Filename: \" + str(name))\n",
        "            print(e)\n",
        "\n",
        "print(\"Unzip successful...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9MLyL8byHmE",
        "outputId": "7b250911-4402-4e4c-fbb2-ed9d161f7784"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A file is corrupted. Filename: wikiart/Baroque/rembrandt_woman-standing-with-raised-hands.jpg\n",
            "Bad CRC-32 for file 'wikiart/Baroque/rembrandt_woman-standing-with-raised-hands.jpg'\n",
            "A file is corrupted. Filename: wikiart/Post_Impressionism/vincent-van-gogh_l-arlesienne-portrait-of-madame-ginoux-1890.jpg\n",
            "Error -3 while decompressing data: invalid block type\n",
            "Unzip successful...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleanup images"
      ],
      "metadata": {
        "id": "GOqy7Uvel427"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "See what dataset_tool.py can do"
      ],
      "metadata": {
        "id": "KsBQgoD3LX-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cmd = f\"/usr/bin/python3 /content/drive/MyDrive/ProjectAmbition/stylegan3/dataset_tool.py --help\"\n",
        "!{cmd}"
      ],
      "metadata": {
        "id": "gXIAsscwLa5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove frames\n",
        "\n",
        "Remove frames if there are any. This process overrides the picture with frames removed."
      ],
      "metadata": {
        "id": "FPXQ-z-SmXdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from scipy.spatial import distance\n",
        "\n",
        "# This python script removes pictures' frames if there are any\n",
        "\n",
        "from_path = '/content/drive/MyDrive/ProjectAmbition/raw_data/'\n",
        "to_path = '/content/drive/MyDrive/ProjectAmbition/raw_data/'\n",
        "\n",
        "\n",
        "def find_left():\n",
        "    left = 0\n",
        "    for i in range(0, w_pad):\n",
        "        r_stdev = np.std(np_img[h_pad:-h_pad, i:i + 1, 0:1])\n",
        "        g_stdev = np.std(np_img[h_pad:-h_pad, i:i + 1, 1:2])\n",
        "        b_stdev = np.std(np_img[h_pad:-h_pad, i:i + 1, 2:3])\n",
        "        if r_stdev * r_stdev + g_stdev * g_stdev + b_stdev * b_stdev > thresh1:\n",
        "            break\n",
        "\n",
        "        r_med = np.median(np_img[h_pad:-h_pad, i:i + 1, 0:1])\n",
        "        g_med = np.median(np_img[h_pad:-h_pad, i:i + 1, 1:2])\n",
        "        b_med = np.median(np_img[h_pad:-h_pad, i:i + 1, 2:3])\n",
        "        dst = distance.euclidean((r_med, g_med, b_med), (r_global_med, g_global_med, b_global_med))\n",
        "        if dst < thresh2:\n",
        "            break\n",
        "\n",
        "        left = left + 1\n",
        "    return left\n",
        "\n",
        "\n",
        "def find_top():\n",
        "    top = 0\n",
        "    for i in range(0, h_pad):\n",
        "        r_stdev = np.std(np_img[i:i + 1, w_pad:-w_pad, 0:1])\n",
        "        g_stdev = np.std(np_img[i:i + 1, w_pad:-w_pad, 1:2])\n",
        "        b_stdev = np.std(np_img[i:i + 1, w_pad:-w_pad, 2:3])\n",
        "        if r_stdev * r_stdev + g_stdev * g_stdev + b_stdev * b_stdev > thresh1:\n",
        "            break\n",
        "\n",
        "        r_med = np.median(np_img[i:i + 1, w_pad:-w_pad, 0:1])\n",
        "        g_med = np.median(np_img[i:i + 1, w_pad:-w_pad, 1:2])\n",
        "        b_med = np.median(np_img[i:i + 1, w_pad:-w_pad, 2:3])\n",
        "        dst = distance.euclidean((r_med, g_med, b_med), (r_global_med, g_global_med, b_global_med))\n",
        "        if dst < thresh2:\n",
        "            break\n",
        "\n",
        "        top = top + 1\n",
        "    return top\n",
        "\n",
        "\n",
        "def find_right(right):\n",
        "    right = w\n",
        "    for i in range(0, w_pad):\n",
        "        r_stdev = np.std(np_img[h_pad:-h_pad, w - i - 1:w - i, 0:1])\n",
        "        g_stdev = np.std(np_img[h_pad:-h_pad, w - i - 1:w - i, 1:2])\n",
        "        b_stdev = np.std(np_img[h_pad:-h_pad, w - i - 1:w - i, 2:3])\n",
        "        if r_stdev * r_stdev + g_stdev * g_stdev + b_stdev * b_stdev > thresh1:\n",
        "            break\n",
        "\n",
        "        r_med = np.median(np_img[h_pad:-h_pad, w - i - 1:w - i, 0:1])\n",
        "        g_med = np.median(np_img[h_pad:-h_pad, w - i - 1:w - i, 1:2])\n",
        "        b_med = np.median(np_img[h_pad:-h_pad, w - i - 1:w - i, 2:3])\n",
        "        dst = distance.euclidean((r_med, g_med, b_med), (r_global_med, g_global_med, b_global_med))\n",
        "        if dst < thresh2:\n",
        "            break\n",
        "\n",
        "        right = right - 1\n",
        "    return right\n",
        "\n",
        "\n",
        "def find_bottom(bottom):\n",
        "    for i in range(0, h_pad):\n",
        "        r_stdev = np.std(np_img[h - i - 1:h - i, w_pad:-w_pad, 0:1])\n",
        "        g_stdev = np.std(np_img[h - i - 1:h - i, w_pad:-w_pad, 1:2])\n",
        "        b_stdev = np.std(np_img[h - i - 1:h - i, w_pad:-w_pad, 2:3])\n",
        "        if r_stdev * r_stdev + g_stdev * g_stdev + b_stdev * b_stdev > thresh1:\n",
        "            break\n",
        "\n",
        "        r_med = np.median(np_img[h - i - 1:h - i, w_pad:-w_pad, 0:1])\n",
        "        g_med = np.median(np_img[h - i - 1:h - i, w_pad:-w_pad, 1:2])\n",
        "        b_med = np.median(np_img[h - i - 1:h - i, w_pad:-w_pad, 2:3])\n",
        "        dst = distance.euclidean((r_med, g_med, b_med), (r_global_med, g_global_med, b_global_med))\n",
        "        if dst < thresh2:\n",
        "            break\n",
        "\n",
        "        bottom = bottom - 1\n",
        "    return bottom\n",
        "\n",
        "\n",
        "for file in os.listdir(from_path):\n",
        "    path = os.path.join(from_path, file)\n",
        "    img = Image.open(path)\n",
        "    file_name, file_extension = os.path.splitext(path)\n",
        "    print('Removing frames for image:', file_name + file_extension)\n",
        "\n",
        "    np_img = np.asarray(img)\n",
        "    # print(\"shape = \" + str(np_img.shape))\n",
        "\n",
        "    thresh1 = 15000\n",
        "    thresh2 = 30\n",
        "    w = img.width\n",
        "    h = img.height\n",
        "    pad = 30\n",
        "    w_pad = w // pad\n",
        "    h_pad = h // pad\n",
        "\n",
        "    r_global_med = np.median(np_img[h_pad:-h_pad, w_pad:-w_pad, 0:1])\n",
        "    g_global_med = np.median(np_img[h_pad:-h_pad, w_pad:-w_pad, 1:2])\n",
        "    b_global_med = np.median(np_img[h_pad:-h_pad, w_pad:-w_pad, 2:3])\n",
        "\n",
        "    left = find_left()\n",
        "    top = find_top()\n",
        "    right = find_right(w)\n",
        "    bottom = find_bottom(h)\n",
        "\n",
        "    # print(\"left = \" + str(left) + \", top = \" + str(top) +\n",
        "      #    \", right = \" + str(right) + \", bottom = \" + str(bottom) + \"\\n\")\n",
        "\n",
        "    # img.save(to_path + file)  # save the original\n",
        "    cropped_img = img.crop((left, top, right, bottom))\n",
        "    cropped_img.save(file_name + file_extension)  # and the cropped version\n"
      ],
      "metadata": {
        "id": "SD-E7AK-mbW2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77ea4e87-a1eb-4ef4-983e-549ab7175cdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removing frames for image: /content/drive/MyDrive/ProjectAmbition/raw_data/peasant-and-horse-1910.jpg\n",
            "Removing frames for image: /content/drive/MyDrive/ProjectAmbition/raw_data/horses.jpg\n",
            "Removing frames for image: /content/drive/MyDrive/ProjectAmbition/raw_data/spring-1907.jpg\n",
            "Removing frames for image: /content/drive/MyDrive/ProjectAmbition/raw_data/the-rise-of-green-square-and-the-woman-s-violin-1916.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Resize images\n",
        "\n",
        "Resize images to 1024x1024 and override them."
      ],
      "metadata": {
        "id": "CW68PWCUqezM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "image_source_path = '/content/drive/MyDrive/ProjectAmbition/raw_data/'\n",
        "image_target_path = '/content/drive/MyDrive/ProjectAmbition/raw_data/'\n",
        "\n",
        "# We are using Pillow to resize all images to our desired size\n",
        "\n",
        "for filename in os.listdir(image_source_path):\n",
        "    path = os.path.join(image_source_path, filename)\n",
        "    image = Image.open(path).resize((1024, 1024), Image.ANTIALIAS)\n",
        "\n",
        "    resized_image = image.save(image_target_path + filename)\n",
        "    print(f\"{filename} image is resized...\")\n"
      ],
      "metadata": {
        "id": "fEiSVY8mqlci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert images\n",
        "\n",
        "Convert raw data images to dataset by using StyleGAN3's built-in dataset_tool.py"
      ],
      "metadata": {
        "id": "mSZS_v2F07l7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/ProjectAmbition/stylegan3/dataset_tool.py --source /content/drive/MyDrive/ProjectAmbition/raw_data/ --dest /content/drive/MyDrive/ProjectAmbition/dataset/ --resolution=1024x1024"
      ],
      "metadata": {
        "id": "nYWi_5DSfKoE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a51e43fc-7f1c-4dd1-cede-39cb82056bd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 4/4 [00:01<00:00,  2.90it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following command can be used to clear out the newly created dataset. If something goes wrong and you need to clean up your images and rerun the above command, you should delete your partially created dataset directory."
      ],
      "metadata": {
        "id": "JfUa1rMSlvTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -R /content/drive/MyDrive/ProjectAmbition/dataset/*"
      ],
      "metadata": {
        "id": "pS0JPUa0lzoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "VIF5keq1x9MA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initial training"
      ],
      "metadata": {
        "id": "Ct68q26xyA2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cmd = f\"/usr/bin/python3 /content/drive/MyDrive/ProjectAmbition/stylegan3/train.py --help\"\n",
        "!{cmd}"
      ],
      "metadata": {
        "id": "txTGnCbG5V0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ninja is a required library. Must be installed"
      ],
      "metadata": {
        "id": "Bmjw3QwfWyWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ninja"
      ],
      "metadata": {
        "id": "2SCfdoIkYfbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.9.0 torchvision==0.10.0 torchaudio==0.9.0 torchtext==0.10.0\n",
        "!pip install transformers==4.8.0"
      ],
      "metadata": {
        "id": "7Of1WYeuYmEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# !export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128\n",
        "\n",
        "# Modify these to suit your needs\n",
        "OUTPUT = \"/content/drive/MyDrive/ProjectAmbition/output\"\n",
        "DATA = \"/content/drive/MyDrive/ProjectAmbition/dataset\"\n",
        "# Snap: How often should the model generate samples and a .pkl file\n",
        "SNAP = 4\n",
        "# Mirrored: Should the images be mirrored left to right?\n",
        "MIRRORED = True\n",
        "\n",
        "\n",
        "# Build the command and run it\n",
        "cmd = f\"/usr/bin/python3 /content/drive/MyDrive/ProjectAmbition/stylegan3/train.py --cfg=stylegan3-t --gpus=1 --batch=8 --gamma=8.2 --snap {SNAP} --outdir {OUTPUT} --data {DATA} --mirror={MIRRORED}\"\n",
        "!{cmd}"
      ],
      "metadata": {
        "id": "Puuif5Y7yBXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Resume training"
      ],
      "metadata": {
        "id": "y8MZ2hfhyB35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "\n",
        "# Modify these to suit your needs\n",
        "OUTPUT = \"/content/drive/MyDrive/ProjectAmbition/output\"\n",
        "NETWORK = \"network-snapshot-000100.pkl\"\n",
        "RESUME = os.path.join(OUTPUT, \"00008-circuit-auto1-resumecustom\", NETWORK)\n",
        "DATA = \"/content/drive/MyDrive/ProjectAmbition/dataset\"\n",
        "# Snap: How often should the model generate samples and a .pkl file\n",
        "SNAP = 4\n",
        "# Mirrored: Should the images be mirrored left to right?\n",
        "MIRRORED = True\n",
        "\n",
        "\n",
        "# Build the command and run it\n",
        "cmd = f\"/usr/bin/python3 /content/drive/MyDrive/ProjectAmbition/stylegan3/train.py --cfg=stylegan3-t --gpus=8 --batch=32 --gamma=8.2 --snap {SNAP} --resume {RESUME} --outdir {OUTPUT} --data {DATA} --mirror={MIRRORED}\"\n",
        "!{cmd}"
      ],
      "metadata": {
        "id": "7fNrhw89yCVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate image"
      ],
      "metadata": {
        "id": "j6-C0KSpyDD8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5KP0UWdqyD49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rgZqHe1eyEWc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}